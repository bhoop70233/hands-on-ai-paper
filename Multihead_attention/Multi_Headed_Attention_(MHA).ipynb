{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tg3Ad2sOYscF"
      },
      "outputs": [],
      "source": [
        "#import necessary librarys\n",
        "import math\n",
        "from typing import Optional,List\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from labml import tracker"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install labml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aykYMiuXZsLa",
        "outputId": "e7c54d00-6eb7-4a5c-d3e0-4767d4f3cf01"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting labml\n",
            "  Downloading labml-0.5.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.10/dist-packages (from labml) (3.1.43)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from labml) (6.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from labml) (1.26.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython->labml) (4.0.11)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython->labml) (5.0.1)\n",
            "Downloading labml-0.5.3-py3-none-any.whl (94 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/94.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m92.2/94.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: labml\n",
            "Successfully installed labml-0.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prepare for multi-head attention"
      ],
      "metadata": {
        "id": "8TTWe8PMZ7y0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class prepareForMultiHeadAttention(nn.Module):\n",
        "  def __init__(self,d_model:int,heads:int,d_k:int,bias:bool):\n",
        "    super().__init__()\n",
        "    #Linear layer for linear transform\n",
        "    self.linear=nn.Linear(d_model,heads*d_k,bias=bias)\n",
        "    #Number of heads\n",
        "    self.heads=heads\n",
        "    #Number of dimension in vectors in each head\n",
        "    self.d_k=d_k\n",
        "\n",
        "  def forward(self,x:torch.Tensor):\n",
        "    head_shape=x.shape[:-1]\n",
        "    #Linear transform\n",
        "    x=self.linear(x)\n",
        "    #split last dimension into heads\n",
        "    x=x.view(*head_shape,self.heads,self.d_k)\n",
        "    return x"
      ],
      "metadata": {
        "id": "6yd06nh3Z2pu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,heads:int,d_model:int,dropout_prob:float=0.1,bias:bool=True):\n",
        "    super().__init__()\n",
        "    #Number of feature per head\n",
        "    self.d_k=d_model//heads\n",
        "    #Number of heads\n",
        "    self.heads=heads\n",
        "    self.query=prepareForMultiHeadAttention(d_model,heads,self.d_k,bias=bias)\n",
        "    self.key=prepareForMultiHeadAttention(d_model,heads,self.d_k,bias=bias)\n",
        "    self.value=prepareForMultiHeadAttention(d_model,heads,self.d_k,bias=True)\n",
        "    #softmax for attention along the time dimension of key\n",
        "    self.softmax=nn.Softmax(dim=1)\n",
        "    #output layer\n",
        "    self.output=nn.Linear(d_model,d_model)\n",
        "    #Dropout\n",
        "    self.dropout=nn.Dropout(dropout_prob)\n",
        "    #scaling factor before the softmax\n",
        "    self.scale=1/math.sqrt(self.d_k)\n",
        "    #we store attentions so that it can be used for logging ,or other computations if needed\n",
        "    self.attn=None\n",
        "\n",
        "    ##calculate scores between queries and keys\n",
        "  def get_scores(self,query:torch.Tensor,key:torch.Tensor):\n",
        "    return torch.einsum('ibhd,jbhd->ijbh',query,key)\n",
        "  def prepare_mask(self,mask:torch.Tensor,query_shape:List[int],key_shape:List[int]):\n",
        "    assert mask.shape[0]==1 or mask.shape[0]==query_shape[0]\n",
        "    assert mask.shape[1]== key_shape[0]\n",
        "    assert mask.shape[2]==1 or mask.shape[2]==query_shape[1]\n",
        "\n",
        "    mask=mask.unsqueeze(-1)\n",
        "    return mask\n",
        "\n",
        "  def forward(self,*,\n",
        "              query:torch.Tensor,\n",
        "              key:torch.Tensor,\n",
        "              value:torch.Tensor,\n",
        "              mask:Optional[torch.Tensor]=None):\n",
        "    seq_len,batch_size, _=query.shape\n",
        "\n",
        "    if mask is not None:\n",
        "      mask=self.prepare_mask(mask,query.shape,key.shape)\n",
        "\n",
        "    query=self.query(query)\n",
        "    key=self.key(key)\n",
        "    value=self.value(value)\n",
        "\n",
        "    scores=self.get_scores(query,key)\n",
        "\n",
        "    scores*=self.scale\n",
        "\n",
        "    #apply mask\n",
        "    if mask is not None:\n",
        "      scores=scores.masked_fill(mask==0,float('-inf'))\n",
        "\n",
        "    attn=self.softmax(scores)\n",
        "    tracker.debug('attn',attn)\n",
        "    #apply dropout\n",
        "    attn=self.dropout(attn)\n",
        "    #multiply by values\n",
        "\n",
        "    x=torch.einsum(\"ijbh,jbhd->ibhd\",attn,value)\n",
        "    self.attn=attn.detach()\n",
        "    #Concatenate multiple heads\n",
        "    x=x.reshape(seq_len,batch_size,-1)\n",
        "    #Output layer\n",
        "    return self.output(x)\n",
        ""
      ],
      "metadata": {
        "id": "hd4B9olEb40g"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1dZRl13deLH9"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}